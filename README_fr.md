# 100 articles de PNL à lire absolument

Il s'agit d'une liste de 100 articles importants sur le traitement du langage naturel (PNL) que les étudiants et les chercheurs sérieux travaillant dans le domaine devraient probablement connaître et lire. Cette liste est compilée par [Masato Hagiwara] (http://masatohagiwara.net/). Je suis heureux de recevoir vos commentaires sur cette liste.

Cette liste est à l'origine basée sur les réponses à une question Quora que j'ai postée il y a des années: [Quels sont les documents de recherche les plus importants que tous les étudiants en PNL devraient absolument lire?] (Https://www.quora.com/What-are-the -les-documents-de-recherche-les-plus-importants-que-tous-les-étudiants-de-PNL-devraient-certainement-lire). Je remercie toutes les personnes qui ont contribué à l'article original.

Cette liste est loin d'être complète ou objective et évolue, car d'importants articles sont publiés année après année. Veuillez me le faire savoir via [pull requests] (https://github.com/mhagiwara/100-nlp-papers/pulls) et [issues] (https://github.com/mhagiwara/100-nlp-papers/issues ) s'il manque quelque chose.

De plus, je n'ai pas essayé d'inclure des liens vers des articles originaux car il faut beaucoup de travail pour maintenir les liens morts à jour. Je suis sûr que vous pouvez trouver la plupart (sinon tous) des articles répertoriés ici via une seule recherche Google par leurs titres.

Un article n'a pas besoin d'être un article de conférence / journal évalué par des pairs pour apparaître ici. Nous incluons également des articles de type didacticiel / enquête et des articles de blog souvent plus faciles à comprendre que les articles originaux.

## Apprentissage automatique

* Avrim Blum et Tom Mitchell: Combinaison de données étiquetées et non étiquetées avec co-formation, 1998.

* John Lafferty, Andrew McCallum, Fernando C.N. Pereira: Champs aléatoires conditionnels: modèles probabilistes pour la segmentation et l'étiquetage des données de séquence, ICML 2001.

* Charles Sutton, Andrew McCallum. Une introduction aux champs aléatoires conditionnels pour l'apprentissage relationnel.

* Kamal Nigam, et al.: Classification de texte à partir de documents étiquetés et non étiquetés utilisant EM. Apprentissage automatique, 1999.

* Kevin Knight: Inférence bayésienne avec larmes, 2009.

* Marco Tulio Ribeiro et al.: "Pourquoi devrais-je vous faire confiance?": Expliquer les prédictions de tout classificateur, KDD 2016.

## Modèles neuronaux

* Richard Socher, et al .: Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection, NIPS 2011.

* Ronan Collobert et al.: Traitement du langage naturel (presque) de Scratch, J. of Machine Learning Research, 2011.

* Richard Socher, et al.: Modèles profonds récursifs pour la composition sémantique sur une banque de sentiments, EMNLP 2013.

* Xiang Zhang, Junbo Zhao et Yann LeCun: Réseaux de convolution au niveau du personnage pour la classification de texte, NIPS 2015.

* Yoon Kim: Réseaux de neurones convolutionnels pour la classification des phrases, 2014.

* Christopher Olah: Comprendre les réseaux LSTM, 2015.

* Matthew E. Peters, et al .: Représentations de mots contextualisés profonds, 2018.

* Jacob Devlin, et al .: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018.

## Clustering et intégration de mots

* Peter F Brown, et al.: Modèles de n-gramme basés sur les classes du langage naturel, 1992.

* Tomas Mikolov, et al .: Estimation efficace des représentations de mots dans l'espace vectoriel, 2013.

* Tomas Mikolov, et al.: Représentations distribuées des mots et des phrases et leur compositionnalité, NIPS 2013.

* Quoc V.Le et Tomas Mikolov: Représentations distribuées de phrases et de documents, 2014.

* Jeffrey Pennington, et al .: GloVe: Global Vectors for Word Representation, 2014.

* Ryan Kiros, et al .: Skip-Thought Vectors, 2015.

* Piotr Bojanowski, et al .: Enriching Word Vectors with Subword Information, 2017.

## Topic Models

* Thomas Hofmann: Indexation sémantique latente probabiliste, SIGIR 1999.

* David Blei, Andrew Y. Ng et Michael I.Jordan: Allocation de dirichlet latent, J. Machine Learning Research, 2003.

## Modélisation du langage

* Joshua Goodman: Un peu de progrès dans la modélisation du langage, Rapport technique MSR, 2001.

* Stanley F. Chen et Joshua Goodman: An Empirical Study of Smoothing Techniques for Language Modeling, ACL 2006.

* Yee Whye Teh: A Hierarchical Bayesian Language Model basé sur les processus Pitman-Yor, COLING / ACL 2006.

* Yee Whye Teh: Une interprétation bayésienne de Interpolated Kneser-Ney, 2006.

* Yoshua Bengio, et al .: A Neural Probabilistic Language Model, J. of Machine Learning Research, 2003.

* Andrej Karpathy: l'efficacité déraisonnable des réseaux de neurones récurrents, 2015.

* Yoon Kim, et al.: Modèles de langage neuronal sensible aux caractères, 2015.

## Segmentation, balisage, analyse

* Donald Hindle et Mats Rooth. Ambiguïté structurelle et relations lexicales, linguistique computationnelle, 1993.

* Adwait Ratnaparkhi: A Maximum Entropy Model for Part-Of-Speech Tagging, EMNLP 1996.

* Eugene Charniak: un analyseur inspiré de l'entropie maximale, NAACL 2000.

* Michael Collins: Méthodes de formation discriminantes pour les modèles de Markov cachés: théorie et expériences avec les algorithmes de Perceptron, EMNLP 2002.

* Dan Klein et Christopher Manning: analyse précise non flexifiée, ACL 2003.

* Dan Klein et Christopher Manning: Induction basée sur le corpus de la structure syntaxique: modèles de dépendance et C
